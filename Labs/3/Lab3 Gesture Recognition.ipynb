{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X6WDvajSqIDs"
   },
   "source": [
    "# Lab 3: Gesture Recognition using Convolutional Neural Networks\n",
    "\n",
    "In this lab you will train a convolutional neural network to make classifications on different hand gestures. By the end of the lab, you should be able to:\n",
    "\n",
    "1. Load and split data for training, validation and testing\n",
    "2. Train a Convolutional Neural Network\n",
    "3. Apply transfer learning to improve your model\n",
    "\n",
    "Note that for this lab we will not be providing you with any starter code. You should be able to take the code used in previous labs, tutorials and lectures and modify it accordingly to complete the tasks outlined below.\n",
    "\n",
    "### What to submit\n",
    "\n",
    "Submit a PDF file containing all your code, outputs, and write-up\n",
    "from parts 1-5. You can produce a PDF of your Google Colab file by\n",
    "going to **File > Print** and then save as PDF. The Colab instructions\n",
    "has more information. Make sure to review the PDF submission to ensure that your answers are easy to read. Make sure that your text is not cut off at the margins. \n",
    "\n",
    "**Do not submit any other files produced by your code.**\n",
    "\n",
    "Include a link to your colab file in your submission.\n",
    "\n",
    "Please use Google Colab to complete this assignment. If you want to use Jupyter Notebook, please complete the assignment and upload your Jupyter Notebook file to Google Colab for submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfiFE_WOqIDu"
   },
   "source": [
    "## Colab Link\n",
    "\n",
    "Include a link to your colab file here\n",
    "\n",
    "Colab Link: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kvTXpH_kqIDy"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "American Sign Language (ASL) is a complete, complex language that employs signs made by moving the\n",
    "hands combined with facial expressions and postures of the body. It is the primary language of many\n",
    "North Americans who are deaf and is one of several communication options used by people who are deaf or\n",
    "hard-of-hearing. The hand gestures representing English alphabet are shown below. This lab focuses on classifying a subset\n",
    "of these hand gesture images using convolutional neural networks. Specifically, given an image of a hand\n",
    "showing one of the letters A-I, we want to detect which letter is being represented.\n",
    "\n",
    "![alt text](https://www.disabled-world.com/pics/1/asl-alphabet.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bJxMgWGNqID2"
   },
   "source": [
    "## Part B. Building a CNN [50 pt]\n",
    "\n",
    "For this lab, we are not going to give you any starter code. You will be writing a convolutional neural network\n",
    "from scratch. You are welcome to use any code from previous labs, lectures and tutorials. You should also\n",
    "write your own code.\n",
    "\n",
    "You may use the PyTorch documentation freely. You might also find online tutorials helpful. However, all\n",
    "code that you submit must be your own.\n",
    "\n",
    "Make sure that your code is vectorized, and does not contain obvious inefficiencies (for example, unecessary\n",
    "for loops, or unnecessary calls to unsqueeze()). Ensure enough comments are included in the code so that\n",
    "your TA can understand what you are doing. It is your responsibility to show that you understand what you\n",
    "write.\n",
    "\n",
    "**This is much more challenging and time-consuming than the previous labs.** Make sure that you\n",
    "give yourself plenty of time by starting early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MiDuQaAh56sT"
   },
   "source": [
    "### 1. Data Loading and Splitting [5 pt]\n",
    "\n",
    "Download the anonymized data provided on Quercus. To allow you to get a heads start on this project we will provide you with sample data from previous years. Split the data into training, validation, and test sets.\n",
    "\n",
    "Note: Data splitting is not as trivial in this lab. We want our test set to closely resemble the setting in which\n",
    "our model will be used. In particular, our test set should contain hands that are never seen in training!\n",
    "\n",
    "Explain how you split the data, either by describing what you did, or by showing the code that you used.\n",
    "Justify your choice of splitting strategy. How many training, validation, and test images do you have?\n",
    "\n",
    "For loading the data, you can use plt.imread as in Lab 1, or any other method that you choose. You may find\n",
    "torchvision.datasets.ImageFolder helpful. (see https://pytorch.org/docs/stable/torchvision/datasets.html?highlight=image%20folder#torchvision.datasets.ImageFolder\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WBrH5kBqRLa6",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images:  1477\n",
      "Number of validation images:  981\n",
      "Number of test images:  1238\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "#Path to dataset\n",
    "data_dir = '/Users/harrynguyen/Documents/GitHub/APS360/Labs/3/Lab3_Gestures_Summer'\n",
    "\n",
    "#Get all individuals \n",
    "individuals = [name for name in os.listdir(data_dir) \n",
    "               if os.path.isdir(os.path.join(data_dir, name))]\n",
    "\n",
    "#Split individuals into train, val, test sets \n",
    "#(30% random for combined validation and test set)\n",
    "train_individuals, test_individuals = train_test_split(individuals, \n",
    "                                                       test_size=0.30, \n",
    "                                                       random_state=42)\n",
    "#(50% random for validation and 50 for test set)\n",
    "val_individuals, test_individuals = train_test_split(individuals, \n",
    "                                                     test_size=0.50, \n",
    "                                                     random_state=42)\n",
    "\n",
    "\n",
    "#create directories for the splits\n",
    "train_dir = '/Users/harrynguyen/Documents/GitHub/APS360/Labs/3/asl_train'\n",
    "val_dir = '/Users/harrynguyen/Documents/GitHub/APS360/Labs/3/asl_val'\n",
    "test_dir = '/Users/harrynguyen/Documents/GitHub/APS360/Labs/3/asl_test'\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "#Define a function to copy images to new directories\n",
    "def copy_images(individuals, source_dir, dest_dir):\n",
    "    for individual in individuals:\n",
    "        individual_dir = os.path.join(source_dir, individual)\n",
    "        if os.path.isdir(individual_dir):\n",
    "            shutil.copytree(individual_dir, os.path.join(dest_dir, \n",
    "                                                         individual), \n",
    "                                                         dirs_exist_ok=True)\n",
    "\n",
    "copy_images(train_individuals, data_dir, train_dir)\n",
    "copy_images(val_individuals, data_dir, val_dir)\n",
    "copy_images(test_individuals, data_dir, test_dir)\n",
    "\n",
    "#Load dataset via ImageFolder\n",
    "train_dataset = ImageFolder(train_dir)\n",
    "val_dataset = ImageFolder(val_dir)\n",
    "test_dataset = ImageFolder(test_dir)\n",
    "\n",
    "#Check output of each set\n",
    "print(f\"Number of training images: \", len(train_dataset))\n",
    "print(f\"Number of validation images: \", len(val_dataset))\n",
    "print(f\"Number of test images: \", len(test_dataset))\n",
    "\n",
    "\n",
    "'''\n",
    "Justification for the splitting procedure\n",
    "\n",
    "- There are two splits, the initial and the further splits. The initial split\n",
    "ensures a clear separation between training data and the data used for \n",
    "validation and testing. The further split ensures that the data used for model \n",
    "evaluation and final performance assessement are distinct.\n",
    "\n",
    "- Initial split comes at 70/30, in which 70% training and 30% combined \n",
    "validation and test. I believe 70% training would ensure enough data for \n",
    "learning without overfitting and the rest 30% would provide enough data \n",
    "for evaluation and tuning its hyperparameters. \n",
    "\n",
    "- During both the tuning and final evaluation phases, having the combined \n",
    "validation and test set equally split ensures a reliable peroformance metrics.\n",
    "\n",
    "However, this is the initial guess and I'll document if any changes would \n",
    "be made.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5VWX4DGY5gQE"
   },
   "source": [
    "### 2. Model Building and Sanity Checking [15 pt]\n",
    "\n",
    "### Part (a) Convolutional Network - 5 pt\n",
    "\n",
    "Build a convolutional neural network model that takes the (224x224 RGB) image as input, and predicts the gesture\n",
    "letter. Your model should be a subclass of nn.Module. Explain your choice of neural network architecture: how\n",
    "many layers did you choose? What types of layers did you use? Were they fully-connected or convolutional?\n",
    "What about other decisions like pooling layers, activation functions, number of channels / hidden units?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2dtx1z5951fS",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. I chose 4 convolutional layers with increasing numbers of filters (64, 128\\n256, 512), as this helps in capturing various levels of abstraction\\n\\n2. I chose 2 separately fully connected layers with 1024 and 512 hidden units,\\nrespectively, allowing for learning combinations of the extracted featuers. \\nThe final fully connected layer maps these features to the output classes \\n(gesture letters).\\n\\n3. Pooling layers: reduce the spatial dimensions by half after each conv. layer\\n\\n4. Activation function: ReLU activation functions introduce non-linearity, which\\nis crucial for learning such patterns. \\n\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ASLClassifier(nn.Module):\n",
    "    def __init__(self, num_classes = 9):\n",
    "        super(ASLClassifier, self).__init__()\n",
    "        self.name = \"ASLClassifier\"\n",
    "\n",
    "        # output (h and w) = (224 - 2*1 + 3) + 1 = 224 = input (h and w)\n",
    "\n",
    "        #Convolutional layers (increasing numbers of filters)\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels=64, \n",
    "                               kernel_size = 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 64, out_channels=128, \n",
    "                               kernel_size = 3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 128, out_channels=256, \n",
    "                               kernel_size = 3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels = 256, out_channels=512, \n",
    "                               kernel_size = 3, stride=1, padding=1)\n",
    "        \n",
    "        #Max-pooling layers \n",
    "        #reducing spatial dimensinons by half after each layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # 224/(2^4) = 14\n",
    "\n",
    "        #Fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=512 * 14 * 14, out_features=1024)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=512)\n",
    "        self.fc3 = nn.Linear(in_features=512, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Convlutional layers with ReLU and pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "\n",
    "        #Flatten the feature map\n",
    "        x = x.view(-1, 512*14*14)\n",
    "\n",
    "        #Fully connected layers with ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) #handled by loss fuction \n",
    "\n",
    "        return x\n",
    "\n",
    "model = ASLClassifier(num_classes=9)\n",
    "\n",
    "'''\n",
    "1. I chose 4 convolutional layers with increasing numbers of filters (64, 128\n",
    "256, 512), as this helps in capturing various levels of abstraction\n",
    "\n",
    "2. I chose 2 separately fully connected layers with 1024 and 512 hidden units,\n",
    "respectively, allowing for learning combinations of the extracted featuers. \n",
    "The final fully connected layer maps these features to the output classes \n",
    "(gesture letters).\n",
    "\n",
    "3. Pooling layers: reduce the spatial dimensions by half after each conv. layer\n",
    "\n",
    "4. Activation function: ReLU activation functions introduce non-linearity, which\n",
    "is crucial for learning such patterns. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XeGvelvb515e"
   },
   "source": [
    "### Part (b) Training Code - 5 pt\n",
    "\n",
    "Write code that trains your neural network given some training data. Your training code should make it easy\n",
    "to tweak the usual hyperparameters, like batch size, learning rate, and the model object itself. Make sure\n",
    "that you are checkpointing your models from time to time (the frequency is up to you). Explain your choice\n",
    "of loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions (Inspired from Lab 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Data Loading\n",
    "\n",
    "def get_relevant_indices(dataset, classes, target_classes):\n",
    "    \"\"\" Return the indices for datapoints in the dataset that belongs to the\n",
    "    desired target classes, a subset of all possible classes.\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset object\n",
    "        classes: A list of strings denoting the name of each class\n",
    "        target_classes: A list of strings denoting the name of desired classes\n",
    "                        Should be a subset of the 'classes'\n",
    "    Returns:\n",
    "        indices: list of indices that have labels corresponding to one of the\n",
    "                 target classes\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    for i in range(len(dataset)):\n",
    "        # Check if the label is in the target classes\n",
    "        label_index = dataset[i][1] # ex: 3\n",
    "        label_class = classes[label_index] # ex: 'cat'\n",
    "        if label_class in target_classes:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "\n",
    "def get_data_loader(target_classes, batch_size):\n",
    "    \"\"\" Loads images of cats and dogs, splits the data into training, validation\n",
    "    and testing datasets. Returns data loaders for the three preprocessed datasets.\n",
    "\n",
    "    Args:\n",
    "        target_classes: A list of strings denoting the name of the desired\n",
    "                        classes. Should be a subset of the argument 'classes'\n",
    "        batch_size: A int representing the number of samples per batch\n",
    "    \n",
    "    Returns:\n",
    "        train_loader: iterable training dataset organized according to batch size\n",
    "        val_loader: iterable validation dataset organized according to batch size\n",
    "        test_loader: iterable testing dataset organized according to batch size\n",
    "        classes: A list of strings denoting the name of each class\n",
    "    \"\"\"\n",
    "\n",
    "    classes = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I')\n",
    "    ########################################################################\n",
    "    # The output of torchvision datasets are PILImage images of range [0, 1].\n",
    "    # We transform them to Tensors of normalized range [-1, 1].\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "         transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "    # Load training data\n",
    "    trainset = datasets.ImageFolder('/Users/harrynguyen/Documents/GitHub/APS360/Labs/3/asl_train',\n",
    "                                    transform=transform)\n",
    "    # Get the list of indices to sample from\n",
    "    relevant_indices = get_relevant_indices(trainset, classes, target_classes)\n",
    "    \n",
    "    # Split into train and validation\n",
    "    np.random.seed(1000) # Fixed numpy random seed for reproducible shuffling\n",
    "    np.random.shuffle(relevant_indices)\n",
    "    split = int(len(relevant_indices) * 0.8) #split at 80%\n",
    "    \n",
    "    # split into training and validation indices\n",
    "    relevant_train_indices, relevant_val_indices = relevant_indices[:split], relevant_indices[split:]  \n",
    "    train_sampler = SubsetRandomSampler(relevant_train_indices)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                               num_workers=1, sampler=train_sampler)\n",
    "    val_sampler = SubsetRandomSampler(relevant_val_indices)\n",
    "    val_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              num_workers=1, sampler=val_sampler)\n",
    "    # Load testing data\n",
    "    testset = datasets.ImageFolder('/Users/harrynguyen/Documents/GitHub/APS360/Labs/3/asl_test',\n",
    "                                   transform=transform)\n",
    "    # Get the list of indices to sample from\n",
    "    relevant_test_indices = get_relevant_indices(testset, classes, target_classes)\n",
    "    test_sampler = SubsetRandomSampler(relevant_test_indices)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             num_workers=1, sampler=test_sampler)\n",
    "    return train_loader, val_loader, test_loader, classes\n",
    "\n",
    "###############################################################################\n",
    "# Training\n",
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
    "\n",
    "    Args:\n",
    "        config: Configuration object containing the hyperparameters\n",
    "    Returns:\n",
    "        path: A string with the hyperparameter name and value concatenated\n",
    "    \"\"\"\n",
    "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch)\n",
    "    return path\n",
    "\n",
    "def normalize_label(labels):\n",
    "    \"\"\"\n",
    "    Given a tensor containing 2 possible values, normalize this to 0/1\n",
    "\n",
    "    Args:\n",
    "        labels: a 1D tensor containing two possible scalar values\n",
    "    Returns:\n",
    "        A tensor normalize to 0/1 value\n",
    "    \"\"\"\n",
    "    max_val = torch.max(labels)\n",
    "    min_val = torch.min(labels)\n",
    "    norm_labels = ((labels - min_val)/(max_val - min_val))\n",
    "    return norm_labels\n",
    "\n",
    "def evaluate(net, loader, criterion):\n",
    "    \"\"\" Evaluate the network on the validation set.\n",
    "\n",
    "     Args:\n",
    "         net: PyTorch neural network object\n",
    "         loader: PyTorch data loader for the validation set\n",
    "         criterion: The loss function\n",
    "     Returns:\n",
    "         err: A scalar for the avg classification error over the validation set\n",
    "         loss: A scalar for the average loss function over the validation set\n",
    "     \"\"\"\n",
    "    net.eval()\n",
    "    total_loss = 0.0\n",
    "    total_err = 0.0\n",
    "    total_epoch = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs, labels = data\n",
    "            labels = normalize_label(labels)\n",
    "            labels = labels.long() \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            corr = (outputs.argmax(dim=1) != labels).sum().item()\n",
    "            total_err += corr\n",
    "            total_loss += loss.item()\n",
    "            total_epoch += len(labels)\n",
    "    err = float(total_err) / total_epoch\n",
    "    loss = float(total_loss) / len(loader)\n",
    "    return err, loss\n",
    "\n",
    "def save_files(model_name, batch_size, learning_rate, epoch, checkpoint_dir, \n",
    "               train_err, train_loss, val_err=None, val_loss=None):\n",
    "    \"\"\"\n",
    "    Saves training and optionally validation error and loss to CSV files.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model.\n",
    "        batch_size (int): Batch size used during training.\n",
    "        learning_rate (float): Learning rate used during training.\n",
    "        epoch (int): Current epoch number.\n",
    "        checkpoint_dir (str): Directory to save the checkpoint files.\n",
    "        train_err (np.array): Array of training errors.\n",
    "        train_loss (np.array): Array of training losses.\n",
    "        val_err (np.array, optional): Array of validation errors.\n",
    "        val_loss (np.array, optional): Array of validation losses.\n",
    "    \"\"\"\n",
    "    model_path = get_model_name(model_name, batch_size, learning_rate, epoch)\n",
    "    \n",
    "    train_err_path = os.path.join(checkpoint_dir, f\"{model_path}_train_err.csv\")\n",
    "    train_loss_path = os.path.join(checkpoint_dir, f\"{model_path}_train_loss.csv\")\n",
    "    np.savetxt(train_err_path, train_err, delimiter=',')\n",
    "    np.savetxt(train_loss_path, train_loss, delimiter=',')\n",
    "    \n",
    "    if val_err is not None and val_loss is not None:\n",
    "        val_err_path = os.path.join(checkpoint_dir, f\"{model_path}_val_err.csv\")\n",
    "        val_loss_path = os.path.join(checkpoint_dir, f\"{model_path}_val_loss.csv\")\n",
    "        np.savetxt(val_err_path, val_err, delimiter=',')\n",
    "        np.savetxt(val_loss_path, val_loss, delimiter=',')\n",
    "\n",
    "###############################################################################\n",
    "# Training Curve\n",
    "def plot_training_curve(path, checkpoint_dir, smalldata = False):\n",
    "    import matplotlib.pyplot as plt\n",
    "    train_err_path = os.path.join(checkpoint_dir, f\"{path}_train_err.csv\")\n",
    "    train_loss_path = os.path.join(checkpoint_dir, f\"{path}_train_loss.csv\")\n",
    "    train_err = np.loadtxt(train_err_path, delimiter= ',')\n",
    "    train_loss = np.loadtxt(train_loss_path, delimiter= ',')\n",
    "    if not smalldata:\n",
    "        val_err_path = os.path.join(checkpoint_dir, f\"{path}_val_err.csv\")\n",
    "        val_loss_path = os.path.join(checkpoint_dir, f\"{path}_val_loss.csv\")\n",
    "        val_err = np.loadtxt(val_err_path, delimiter= ',')\n",
    "        val_loss = np.loadtxt(val_loss_path, delimiter= ',')\n",
    "        plt.title(\"Train vs Validation Error\")\n",
    "        n = len(train_err) # number of epochs\n",
    "        plt.plot(range(1,n+1), train_err, label=\"Train\")\n",
    "        plt.plot(range(1,n+1), val_err, label=\"Validation\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "        plt.title(\"Train vs Validation Loss\")\n",
    "        plt.plot(range(1,n+1), train_loss, label=\"Train\")\n",
    "        plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.title(\"Train Error and Train Loss\")\n",
    "        n = len(train_err)\n",
    "        plt.plot(range(1,n+1), train_err, label=\"Train Error\")\n",
    "        plt.plot(range(1,n+1), train_loss, label=\"Train Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Value Ratio\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "17YTQv4l54W1",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nChoice of Loss Function: Cross-Entroypy Loss\\n--> Chosen for its suitability for multi-class classification tasks,\\nproviding a measure of how well the predicted probabilities match \\nthe true class labels\\n\\nChoice of Optimizer: Adam\\n--> Chosen for its adaptive learning rate, combining the benefits of \\nAdaGrad and RMSProp, making it effective and efficient for training neural\\nnetworks'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model(net, batch_size, learning_rate, num_epochs, checkpoint_frequency):\n",
    "    torch.manual_seed(1000)\n",
    "\n",
    "    train_loader, val_loader, test_loader, classes = get_data_loader(\n",
    "            target_classes=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\"], batch_size=batch_size)\n",
    "    \n",
    "    # Define the Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_err = np.zeros(num_epochs)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    val_err = np.zeros(num_epochs)\n",
    "    val_loss = np.zeros(num_epochs)\n",
    "    \n",
    "    # Create directories for checkpoints if they don't exist\n",
    "    checkpoint_dir = '/Users/harrynguyen/Documents/GitHub/APS360/Labs/3/model_checkpoint'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    start_time = time.time()\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0.0\n",
    "        total_train_err = 0.0\n",
    "        total_epoch = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            labels = labels.long()  # Convert labels to the required format\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            corr = (outputs.argmax(dim=1) != labels).sum().item()\n",
    "            total_train_err += corr\n",
    "            total_train_loss += loss.item()\n",
    "            total_epoch += len(labels)\n",
    "        \n",
    "        train_err[epoch] = float(total_train_err) / total_epoch\n",
    "        train_loss[epoch] = float(total_train_loss) / (i + 1)\n",
    "        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)\n",
    "        print((\"Epoch {}: Train err: {}, Train loss: {} |\"+\n",
    "               \"Validation err: {}, Validation loss: {}\").format(\n",
    "                   epoch + 1,\n",
    "                   train_err[epoch],\n",
    "                   train_loss[epoch],\n",
    "                   val_err[epoch],\n",
    "                   val_loss[epoch]))\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % checkpoint_frequency == 0:\n",
    "            save_files(net.name, batch_size, learning_rate, epoch, checkpoint_dir,\n",
    "                       train_err, train_loss, val_err, val_loss)\n",
    "            print(f\"Model checkpoint saved at {epoch + 1}th epoch\")\n",
    "    \n",
    "    print('Finished Training')\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Total time elapsed: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    # # Save metrics to CSV\n",
    "    # epochs = np.arange(1, num_epochs + 1)\n",
    "    # np.savetxt(f\"{checkpoint_dir}/train_err.csv\", train_err, delimiter=',')\n",
    "    # np.savetxt(f\"{checkpoint_dir}/train_loss.csv\", train_loss, delimiter=',')\n",
    "    # np.savetxt(f\"{checkpoint_dir}/val_err.csv\", val_err, delimiter=',')\n",
    "    # np.savetxt(f\"{checkpoint_dir}/val_loss.csv\", val_loss, delimiter=',')\n",
    "    \n",
    "    return net, train_err, train_loss, val_err, val_loss\n",
    "\n",
    "\n",
    "'''\n",
    "Choice of Loss Function: Cross-Entroypy Loss\n",
    "--> Chosen for its suitability for multi-class classification tasks,\n",
    "providing a measure of how well the predicted probabilities match \n",
    "the true class labels\n",
    "\n",
    "Choice of Optimizer: Adam\n",
    "--> Chosen for its adaptive learning rate, combining the benefits of \n",
    "AdaGrad and RMSProp, making it effective and efficient for training neural\n",
    "networks'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bk1RNgAj54rZ"
   },
   "source": [
    "### Part (c) “Overfit” to a Small Dataset - 5 pt\n",
    "\n",
    "One way to sanity check our neural network model and training code is to check whether the model is capable\n",
    "of “overfitting” or “memorizing” a small dataset. A properly constructed CNN with correct training code\n",
    "should be able to memorize the answers to a small number of images quickly.\n",
    "\n",
    "Construct a small dataset (e.g. just the images that you have collected). Then show that your model and\n",
    "training code is capable of memorizing the labels of this small data set.\n",
    "\n",
    "With a large batch size (e.g. the entire small dataset) and learning rate that is not too high, You should be\n",
    "able to obtain a 100% training accuracy on that small dataset relatively quickly (within 200 iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "lXYRBhQO6d3u",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_small_data_loader(batch_size):\n",
    "    classes = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I')\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    #Loadd small dataset\n",
    "    small_dataset = datasets.ImageFolder('/Users/harrynguyen/Documents/GitHub/APS360/Labs/3/small_dataset',\n",
    "                                         transform=transform)\n",
    "    small_loader = torch.utils.data.DataLoader(small_dataset, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True,\n",
    "                                                num_workers=1)\n",
    "    return small_loader, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_small_dataset(net, batch_size = 8, learning_rate=0.001, num_epochs=20):\n",
    "    # Fixed PyTorch random seed for reproducibility\n",
    "    torch.manual_seed(1000)\n",
    "    \n",
    "    # Obtain the small dataset loader\n",
    "    small_loader, classes = get_small_data_loader(batch_size=batch_size)\n",
    "    \n",
    "    # Define the Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_err = np.zeros(num_epochs)\n",
    "    train_loss = np.zeros(num_epochs)\n",
    "    \n",
    "    checkpoint_dir = '/Users/harrynguyen/Documents/GitHub/APS360/Labs/3/small_checkpoint'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0.0\n",
    "        total_train_err = 0.0\n",
    "        total_epoch = 0\n",
    "        \n",
    "        for i, data in enumerate(small_loader, 0):\n",
    "            inputs, labels = data\n",
    "            labels = labels.long()  # Convert labels to Long type\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            corr = (outputs.argmax(dim=1) != labels).sum().item()\n",
    "            total_train_err += corr\n",
    "            total_train_loss += loss.item()\n",
    "            total_epoch += len(labels)\n",
    "        \n",
    "        train_err[epoch] = float(total_train_err) / total_epoch\n",
    "        train_loss[epoch] = float(total_train_loss) / (i + 1)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}: Train err: {train_err[epoch]}, Train loss: {train_loss[epoch]}\")\n",
    "        \n",
    "        # Check if training error is zero (i.e., model has overfitted)\n",
    "        if train_err[epoch] == 0.0:\n",
    "            print(f\"Model has memorized the dataset in {epoch + 1} epochs.\")\n",
    "            break\n",
    "    \n",
    "        # model_path = get_model_name(net.name, batch_size, learning_rate, epoch)\n",
    "        # torch.save(net.state_dict(), model_path)\n",
    "    print('Finished Training on Small Dataset')\n",
    "\n",
    "    #Save metrics to CSV\n",
    "    model_path = get_model_name(net.name, batch_size, learning_rate, epoch)\n",
    "    train_err_path = os.path.join(checkpoint_dir, f\"{model_path}_train_err.csv\")\n",
    "    train_loss_path = os.path.join(checkpoint_dir, f\"{model_path}_train_loss.csv\")\n",
    "    epochs = np.arange(1, num_epochs + 1)\n",
    "    np.savetxt(train_err_path, train_err, delimiter= ',')\n",
    "    np.savetxt(train_loss_path, train_loss, delimiter= ',')\n",
    "\n",
    "    return net, train_err, train_loss\n",
    "\n",
    "\n",
    "# Example of how to call train_on_small_dataset\n",
    "# Assume net is an instance of the ASLClassifier model\n",
    "# train_on_small_dataset(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train err: 0.8888888888888888, Train loss: 2.5234713554382324\n",
      "Epoch 2: Train err: 0.8888888888888888, Train loss: 2.2069159348805747\n",
      "Epoch 3: Train err: 0.9111111111111111, Train loss: 2.1965991059939065\n",
      "Epoch 4: Train err: 0.8222222222222222, Train loss: 2.063323676586151\n",
      "Epoch 5: Train err: 0.6444444444444445, Train loss: 1.8076063593228657\n",
      "Epoch 6: Train err: 0.45555555555555555, Train loss: 1.3915549019972484\n",
      "Epoch 7: Train err: 0.28888888888888886, Train loss: 1.3775867174069087\n",
      "Epoch 8: Train err: 0.3, Train loss: 0.7074908781796694\n",
      "Epoch 9: Train err: 0.28888888888888886, Train loss: 0.7543661370873451\n",
      "Epoch 10: Train err: 0.15555555555555556, Train loss: 0.3971026645352443\n",
      "Epoch 11: Train err: 0.13333333333333333, Train loss: 0.31333390654375154\n",
      "Epoch 12: Train err: 0.06666666666666667, Train loss: 0.15863611735403538\n",
      "Epoch 13: Train err: 0.03333333333333333, Train loss: 0.07195909054280492\n",
      "Epoch 14: Train err: 0.07777777777777778, Train loss: 0.5381678578123683\n",
      "Epoch 15: Train err: 0.1111111111111111, Train loss: 0.29668988784154254\n",
      "Epoch 16: Train err: 0.05555555555555555, Train loss: 0.16076748252574666\n",
      "Epoch 17: Train err: 0.08888888888888889, Train loss: 0.2771379967064907\n",
      "Epoch 18: Train err: 0.15555555555555556, Train loss: 0.9676114555137852\n",
      "Epoch 19: Train err: 0.06666666666666667, Train loss: 0.3618133223305146\n",
      "Epoch 20: Train err: 0.044444444444444446, Train loss: 0.09896044247579994\n",
      "Finished Training on Small Dataset\n"
     ]
    }
   ],
   "source": [
    "small_model = ASLClassifier()\n",
    "small_model, train_err, train_loss = train_on_small_dataset(small_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "small_model_path = get_model_name(small_model.name, 8, 0.001, 19)\n",
    "checkpoint_dir = '/Users/harrynguyen/Documents/GitHub/APS360/Labs/3/small_checkpoint'\n",
    "plot_training_curve(small_model_path, checkpoint_dir, smalldata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nvDLw-Vz6eVS"
   },
   "source": [
    "### 3. Hyperparameter Search [15 pt]\n",
    "\n",
    "### Part (a) - 3 pt\n",
    "\n",
    "List 3 hyperparameters that you think are most worth tuning. Choose at least one hyperparameter related to\n",
    "the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HXRQbgMqR_Qy",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "3 hyperpameters that I think are most worth tuning are:\n",
    "- Learning Rate: determines the size of the steps taken during gradient\n",
    "                 descent to minimize loss function. Tuning range is \n",
    "                 between 10^-3 to 10^-1.\n",
    "- Batch Size:    defines the number of samples that will be propagated\n",
    "                 through the network at one time. Common values (used\n",
    "                 for tuning) are 16, 32, 64, 128, etc.\n",
    "- Number of Filters in Convolutional Layers: \n",
    "                 This hyperparameter is related to the model architecture.\n",
    "                 It affects the capacity of the network to learn different \n",
    "                 features from the input data. Common values are the power\n",
    "                 of 2, such as 32, 64, 128, etc.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zeD6EzPB6kSW"
   },
   "source": [
    "### Part (b) - 5 pt\n",
    "\n",
    "Tune the hyperparameters you listed in Part (a), trying as many values as you need to until you feel satisfied\n",
    "that you are getting a good model. Plot the training curve of at least 4 different hyperparameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UkvdR-cB6nzm",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate_model(learning_rate, batch_size, num_filters, epochs, checkpoint_frequency):\n",
    "    class CustomASLClassifier(nn.Module):\n",
    "        def __init__(self, num_classes=9):\n",
    "            super(CustomASLClassifier, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, num_filters, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv2 = nn.Conv2d(num_filters, num_filters*2, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv3 = nn.Conv2d(num_filters*2, num_filters*4, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv4 = nn.Conv2d(num_filters*4, num_filters*8, kernel_size=3, stride=1, padding=1)\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "            self.fc1 = nn.Linear(num_filters*8 * 14 * 14, 1024)\n",
    "            self.fc2 = nn.Linear(1024, 512)\n",
    "            self.fc3 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            x = self.pool(F.relu(self.conv3(x)))\n",
    "            x = self.pool(F.relu(self.conv4(x)))\n",
    "            x = x.view(-1, num_filters*8 * 14 * 14)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "    # Initialize model, train and evaluate\n",
    "    model = CustomASLClassifier()\n",
    "    train_err, train_loss, val_err, val_loss = train_model(model, batch_size, learning_rate, epochs, \n",
    "                                                           checkpoint_frequency)\n",
    "    \n",
    "    return train_err, train_loss, val_err, val_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train and evaluate models with different hyperparameter settings\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m params \u001b[38;5;129;01min\u001b[39;00m hyperparams:\n\u001b[0;32m---> 13\u001b[0m     train_err, train_loss, val_err, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_filters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: params,\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_err\u001b[39m\u001b[38;5;124m'\u001b[39m: train_err,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: val_loss\n\u001b[1;32m     22\u001b[0m     })\n",
      "Cell \u001b[0;32mIn[99], line 34\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(learning_rate, batch_size, num_filters, epochs, checkpoint_frequency)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Initialize model, train and evaluate\u001b[39;00m\n\u001b[1;32m     33\u001b[0m model \u001b[38;5;241m=\u001b[39m CustomASLClassifier()\n\u001b[0;32m---> 34\u001b[0m train_err, train_loss, val_err, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mcheckpoint_frequency\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_err, train_loss, val_err, val_loss\n",
      "Cell \u001b[0;32mIn[108], line 44\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(net, batch_size, learning_rate, num_epochs, checkpoint_frequency)\u001b[0m\n\u001b[1;32m     42\u001b[0m train_err[epoch] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(total_train_err) \u001b[38;5;241m/\u001b[39m total_epoch\n\u001b[1;32m     43\u001b[0m train_loss[epoch] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(total_train_loss) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m val_err[epoch], val_loss[epoch] \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: Train err: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Train loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m |\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     46\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation err: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Validation loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     47\u001b[0m            epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m            val_err[epoch],\n\u001b[1;32m     51\u001b[0m            val_loss[epoch]))\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Save checkpoint\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[107], line 130\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(net, loader, criterion)\u001b[0m\n\u001b[1;32m    128\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mlong() \n\u001b[1;32m    129\u001b[0m outputs \u001b[38;5;241m=\u001b[39m net(inputs)\n\u001b[0;32m--> 130\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m corr \u001b[38;5;241m=\u001b[39m (outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    132\u001b[0m total_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m corr\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "# Hyperparameter settings to try\n",
    "hyperparams = [\n",
    "    {'learning_rate': 0.001, 'batch_size': 32, 'num_filters': 32},\n",
    "    {'learning_rate': 0.001, 'batch_size': 64, 'num_filters': 64},\n",
    "    {'learning_rate': 0.01, 'batch_size': 64, 'num_filters': 64},\n",
    "    {'learning_rate': 0.01, 'batch_size': 128, 'num_filters': 128},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Train and evaluate models with different hyperparameter settings\n",
    "for params in hyperparams:\n",
    "    train_err, train_loss, val_err, val_loss = train_and_evaluate_model(\n",
    "        params['learning_rate'], params['batch_size'], params['num_filters'],\n",
    "        epochs=20, checkpoint_frequency=5)\n",
    "    results.append({\n",
    "        'params': params,\n",
    "        'train_err': train_err,\n",
    "        'train_loss': train_loss,\n",
    "        'val_err': val_err,\n",
    "        'val_loss': val_loss\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "for result in results:\n",
    "    params = result['params']\n",
    "    train_err = result['train_err']\n",
    "    train_loss = result['train_loss']\n",
    "    val_err = result['val_err']\n",
    "    val_loss = result['val_loss']\n",
    "    \n",
    "    epochs = np.arange(1, len(train_err) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, label='Train Loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Learning Rate: {params[\"learning_rate\"]}, Batch Size: {params[\"batch_size\"]}, Filters: {params[\"num_filters\"]}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_err, label='Train Error')\n",
    "    plt.plot(epochs, val_err, label='Validation Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title(f'Learning Rate: {params[\"learning_rate\"]}, Batch Size: {params[\"batch_size\"]}, Filters: {params[\"num_filters\"]}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H93iN5_l60BO"
   },
   "source": [
    "### Part (c) - 3 pt\n",
    "Choose the best model out of all the ones that you have trained. Justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kIhcN7IG6zRO",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QzNA5oup67JO"
   },
   "source": [
    "### Part (d) - 4 pt\n",
    "Report the test accuracy of your best model. You should only do this step once and prior to this step you should have only used the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2eJ7AbVl6-ax",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wrem-iXV6_Bz"
   },
   "source": [
    "### 4. Transfer Learning [15 pt]\n",
    "For many image classification tasks, it is generally not a good idea to train a very large deep neural network\n",
    "model from scratch due to the enormous compute requirements and lack of sufficient amounts of training\n",
    "data.\n",
    "\n",
    "One of the better options is to try using an existing model that performs a similar task to the one you need\n",
    "to solve. This method of utilizing a pre-trained network for other similar tasks is broadly termed **Transfer\n",
    "Learning**. In this assignment, we will use Transfer Learning to extract features from the hand gesture\n",
    "images. Then, train a smaller network to use these features as input and classify the hand gestures.\n",
    "\n",
    "As you have learned from the CNN lecture, convolution layers extract various features from the images which\n",
    "get utilized by the fully connected layers for correct classification. AlexNet architecture played a pivotal\n",
    "role in establishing Deep Neural Nets as a go-to tool for image classification problems and we will use an\n",
    "ImageNet pre-trained AlexNet model to extract features in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rWdQJz4Q7O2F"
   },
   "source": [
    "### Part (a) - 5 pt\n",
    "Here is the code to load the AlexNet network, with pretrained weights. When you first run the code, PyTorch\n",
    "will download the pretrained weights from the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "BJKcTW9C7TZk",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /Users/harrynguyen/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
      "100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models\n",
    "alexnet = torchvision.models.alexnet(pretrained=True)\n",
    "alexnet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NQ0GZYaP7VAR"
   },
   "source": [
    "The alexnet model is split up into two components: *alexnet.features* and *alexnet.classifier*. The\n",
    "first neural network component, *alexnet.features*, is used to compute convolutional features, which are\n",
    "taken as input in *alexnet.classifier*.\n",
    "\n",
    "The neural network alexnet.features expects an image tensor of shape Nx3x224x224 as input and it will\n",
    "output a tensor of shape Nx256x6x6 . (N = batch size).\n",
    "\n",
    "Compute the AlexNet features for each of your training, validation, and test data. Here is an example code\n",
    "snippet showing how you can compute the AlexNet features for some images (your actual code might be\n",
    "different):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oX7SjVdB7XAE",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load the datasets\n",
    "train_dataset = datasets.ImageFolder('/Users/harrynguyen/Documents/GitHub/APS360/Labs/3/asl_train', transform=transform)\n",
    "val_dataset = datasets.ImageFolder('/Users/harrynguyen/Documents/GitHub/APS360/Labs/3/asl_val', transform=transform)\n",
    "test_dataset = datasets.ImageFolder('/Users/harrynguyen/Documents/GitHub/APS360/Labs/3/asl_test', transform=transform)\n",
    "\n",
    "# Define the data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DYcjHg_A7cCM"
   },
   "source": [
    "**Save the computed features**. You will be using these features as input to your neural network in Part\n",
    "(b), and you do not want to re-compute the features every time. Instead, run *alexnet.features* once for\n",
    "each image, and save the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "TBo1BpL373LX",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_and_save_features(dataloader, model, feature_file, label_file):\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model.features(inputs)\n",
    "            features.append(outputs)\n",
    "            labels.append(targets)\n",
    "    features = torch.cat(features)\n",
    "    labels = torch.cat(labels)\n",
    "    torch.save(features, feature_file)\n",
    "    torch.save(labels, label_file)\n",
    "\n",
    "# Extract and save features for each dataset\n",
    "extract_and_save_features(train_loader, alexnet, 'train_features.pt', 'train_labels.pt')\n",
    "extract_and_save_features(val_loader, alexnet, 'val_features.pt', 'val_labels.pt')\n",
    "extract_and_save_features(test_loader, alexnet, 'test_features.pt', 'test_labels.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OFWvvhFN73qY"
   },
   "source": [
    "### Part (b) - 3 pt\n",
    "Build a convolutional neural network model that takes as input these AlexNet features, and makes a\n",
    "prediction. Your model should be a subclass of nn.Module.\n",
    "\n",
    "Explain your choice of neural network architecture: how many layers did you choose? What types of layers\n",
    "did you use: fully-connected or convolutional? What about other decisions like pooling layers, activation\n",
    "functions, number of channels / hidden units in each layer?\n",
    "\n",
    "Here is an example of how your model may be called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oVTuHUeV78-U",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nExplanation of Choices:\\n\\nFully Connected Layers:\\n - First Fully Connected Layer: Reduces the dimensionality \\nfrom 9216 (256 * 6 * 6) to 1024. This layer helps in \\nlearning complex representations of the input features.\\n - Second Fully Connected Layer: Maps the 1024 features \\n to the number of classes (9). This layer provides the \\n final output logits for classification.\\n\\nActivation Function (ReLU):\\n- ReLU is used after the first fully connected layer \\n to introduce non-linearity. This activation function \\n helps the model learn complex patterns and prevents issues \\n related to vanishing gradients.\\n\\nFlattening the Input:\\n - The input features from AlexNet are in the shape \\n [N, 256, 6, 6]. Before feeding them into the fully \\n connected layers, we flatten these features to a 1D \\n tensor of size 9216 (256 * 6 * 6).\\n\\nSoftmax Function:\\n - Although not included in the model definition, the \\n softmax function is applied to the output logits to get \\n the probability distribution over the classes. This is \\n useful for interpreting the model's predictions.\\n\""
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AlexNetFeatureClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=9):\n",
    "        super(AlexNetFeatureClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(3*224*224, 256*6*6)\n",
    "        self.fc2 = nn.Linear(256*6*6,1024)\n",
    "        self.fc3 = nn.Linear(1024, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape before flattening: {x.shape}\")\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        print(f\"Shape after flattening: {x.shape}\")\n",
    "        x = F.relu(self.fc1(x))    # First fully connected layer with ReLU\n",
    "        x = F.relu(self.fc2(x))    # Second fully connected layer\n",
    "        x = self.fc3(x)            \n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "# features = ...  # Load precomputed AlexNet features\n",
    "# model = AlexNetFeatureClassifier(num_classes=9)\n",
    "# output = model(features)\n",
    "# prob = F.softmax(output, dim=1)\n",
    "\n",
    "'''\n",
    "Explanation of Choices:\n",
    "\n",
    "Fully Connected Layers:\n",
    " - First Fully Connected Layer: Reduces the dimensionality \n",
    "from 9216 (256 * 6 * 6) to 1024. This layer helps in \n",
    "learning complex representations of the input features.\n",
    " - Second Fully Connected Layer: Maps the 1024 features \n",
    " to the number of classes (9). This layer provides the \n",
    " final output logits for classification.\n",
    "\n",
    "Activation Function (ReLU):\n",
    "- ReLU is used after the first fully connected layer \n",
    " to introduce non-linearity. This activation function \n",
    " helps the model learn complex patterns and prevents issues \n",
    " related to vanishing gradients.\n",
    "\n",
    "Flattening the Input:\n",
    " - The input features from AlexNet are in the shape \n",
    " [N, 256, 6, 6]. Before feeding them into the fully \n",
    " connected layers, we flatten these features to a 1D \n",
    " tensor of size 9216 (256 * 6 * 6).\n",
    "\n",
    "Softmax Function:\n",
    " - Although not included in the model definition, the \n",
    " softmax function is applied to the output logits to get \n",
    " the probability distribution over the classes. This is \n",
    " useful for interpreting the model's predictions.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wVAGuURu7-9q"
   },
   "source": [
    "### Part (c) - 5 pt\n",
    "Train your new network, including any hyperparameter tuning. Plot and submit the training curve of your\n",
    "best model only.\n",
    "\n",
    "Note: Depending on how you are caching (saving) your AlexNet features, PyTorch might still be tracking\n",
    "updates to the **AlexNet weights**, which we are not tuning. One workaround is to convert your AlexNet\n",
    "feature tensor into a numpy array, and then back into a PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JCmiH11x7-q1",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load the precomputed features and labels\n",
    "train_features = torch.load('train_features.pt')\n",
    "train_labels = torch.load('train_labels.pt')\n",
    "val_features = torch.load('val_features.pt')\n",
    "val_labels = torch.load('val_labels.pt')\n",
    "test_features = torch.load('test_features.pt')\n",
    "test_labels = torch.load('test_labels.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor(tensor):\n",
    "    return torch.from_numpy(tensor.detach().numpy())\n",
    "\n",
    "train_features = convert_to_tensor(train_features)\n",
    "train_labels = convert_to_tensor(train_labels)\n",
    "val_features = convert_to_tensor(val_features)\n",
    "val_labels = convert_to_tensor(val_labels)\n",
    "test_features = convert_to_tensor(test_features)\n",
    "test_labels = convert_to_tensor(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset = TensorDataset(val_features, val_labels)\n",
    "test_dataset = TensorDataset(test_features, test_labels)\n",
    "\n",
    "#Data Loader already defined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape before flattening: torch.Size([64, 3, 224, 224])\n",
      "Shape after flattening: torch.Size([64, 150528])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.12.1' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model_alex(net, train_loader, val_loader, num_epochs, learning_rate):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        net.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Initialize and train the model\n",
    "model = AlexNetFeatureClassifier(num_classes=9)\n",
    "train_losses, val_losses = train_model_alex(model, train_loader, val_loader, num_epochs=30, learning_rate=0.001)\n",
    "\n",
    "# Plot the training curve\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQ2tvqJ68Mqb"
   },
   "source": [
    "### Part (d) - 2 pt\n",
    "Report the test accuracy of your best model. How does the test accuracy compare to Part 3(d) without transfer learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yCp_kFSg8Q2T",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 3 - Gesture Recognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
